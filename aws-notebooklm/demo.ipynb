{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c518373d-ec80-4941-a3c7-a13de52403dd",
   "metadata": {},
   "source": [
    "## Notebook LM with Amazon Bedrock Claude Sonnet\n",
    "This notebook is inspired by Audio overview feature of Google Notebook LM. It generates podcast style audio discussions of uploaded PDF document. \n",
    "\n",
    "You can upload any PDF document, extracts the content organizes them into interesting themes and generates questions and answers between podcast host and a guest (usually the document author). \n",
    "\n",
    "Below is the solution architecture. \n",
    "\n",
    "<img src=\"images/aws-notebooklm-architecture.png\" width=500>\n",
    "\n",
    "You can upload any PDF file. Data is extratced with pypdf library, cleaned-up with custom regex expressions. This is sent to Amazon Bedrock Claude 3.0 Sonnet model with a custom prompt that generates a podcast transcript. The transcript is split as segments. These audio segments are passed to Amazon Polly, a text-to-speech service, to generate the audio clips for each segment of the discussion. Finally with pydub, segments are combined as a single audio file.  \n",
    "\n",
    "Amazon Polly enables existing applications to speak as a first class feature and creates the opportunity for entirely new categories of speech-enabled products, from mobile apps and cars, to devices and appliances. Amazon Polly includes dozens of lifelike voices and support for multiple languages, so you can select the ideal voice and distribute your speech-enabled applications in many geographies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61315474-2eb1-42c7-b668-c97d4bd77829",
   "metadata": {},
   "source": [
    "### Install dependecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96ec58ec-05e5-48d5-aa0d-5474caee10c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.10/site-packages (1.35.34)\n",
      "Requirement already satisfied: botocore<1.36.0,>=1.35.34 in /opt/conda/lib/python3.10/site-packages (from boto3) (1.35.34)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from boto3) (0.10.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.36.0,>=1.35.34->boto3) (2.9.0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.10/site-packages (from botocore<1.36.0,>=1.35.34->boto3) (1.26.19)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.36.0,>=1.35.34->boto3) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install boto3 --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79465627-099e-4fa2-b660-4b4c5466a014",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pypdf --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc1d883-9787-4c02-8ab8-008d1ae04716",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pydub --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5ca0ec-4bdb-4533-ba41-66459eea3612",
   "metadata": {},
   "source": [
    "### Install ffmpeg & check\n",
    "sudo apt-get update\n",
    "sudo apt-get install ffmpeg\n",
    "\n",
    "ffmpeg -version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc19ae2a-2408-4ef6-8afb-bbeddd6fc864",
   "metadata": {},
   "source": [
    "### Restart Kernel after the installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b266b94-4531-45e4-970d-f2657293538e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75e06a38-a81d-4d7c-8ece-42554dc6ce98",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import utils\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a005ebf-374c-4f3a-85f4-6249e8e50dc9",
   "metadata": {},
   "source": [
    "### Process PDF, generate transcript and audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02011938-bfba-4967-8cae-dce89d48eff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a600d49-ce09-4081-8f28-c02953dc6ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_audio_dir(\"audio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c164ae7a-b60d-4e8a-be3b-2e36a02d7367",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pdf_to_text(\"pdfs/sample.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8d908a7-e0da-4d57-ab64-60ecef6e3292",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = get_system_prompt(host_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a4c9b14-db5d-488f-871e-0a5e686d9212",
   "metadata": {},
   "outputs": [],
   "source": [
    "guest_name,title,extracted_speech = generate_podcast_script(system_prompt,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a617f825-db63-4cd0-8508-5040ca63083d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Ashish Vaswani',\n",
       " 'Exploring the Transformer: A Revolutionary Attention-Based Model')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guest_name, title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4b6c96d-ca8d-41d2-8413-e57869790184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'speaker': 'Amy',\n",
       "  'speech': '<speak>OK listeners, today we have a fascinating discussion on the Transformer, a groundbreaking neural network architecture that has revolutionized sequence modeling tasks like machine translation. Joining me is Ashish Vaswani, one of the lead researchers behind this innovative model from Google Brain. Welcome Ashish!</speak>'},\n",
       " {'speaker': 'Ashish Vaswani',\n",
       "  'speech': \"<speak>Thanks for having me Amy. I'm excited to share insights about the Transformer and how it departs from traditional recurrent and convolutional models by relying entirely on an attention mechanism.</speak>\"},\n",
       " {'speaker': 'Amy',\n",
       "  'speech': \"<speak>Absolutely, let's dive right in. The Transformer replaces the recurrent layers commonly used in sequence models with self-attention layers. Can you explain what self-attention is and how it works in this context?</speak>\"},\n",
       " {'speaker': 'Ashish Vaswani',\n",
       "  'speech': \"<speak>Certainly. <break time='1s'/> Self-attention is a mechanism that allows each part of the input sequence to attend to other parts of the same sequence to compute representations. In the Transformer, we use multi-head self-attention where the model jointly attends to information from different representation subspaces at different positions. This allows the model to capture long-range dependencies more directly compared to recurrent or convolutional models.</speak>\"},\n",
       " {'speaker': 'Amy',\n",
       "  'speech': \"<speak>That's really fascinating. <break time='1s'/> And I understand the Transformer achieves state-of-the-art results on machine translation tasks while being more parallelizable and faster to train than previous models. Can you elaborate on those performance gains?</speak>\"},\n",
       " {'speaker': 'Ashish Vaswani',\n",
       "  'speech': \"<speak>Absolutely Amy. On the WMT 2014 English-to-German translation task, our big Transformer model achieved a new state-of-the-art BLEU score of 28.4, outperforming even the best ensemble models from the literature by over 2 BLEU points. <break time='1s'/> And on English-to-French, we established a new single-model record of 41.8 BLEU after training for just 3.5 days on 8 GPUs. The self-attention layers allow for much more parallelization compared to recurrent models, drastically reducing training time.</speak>\"},\n",
       " {'speaker': 'Amy',\n",
       "  'speech': \"<speak>Wow, those are impressive results! <break time='1s'/> I'm also curious about the attention visualizations you included in the paper. It seems like the model is learning to attend to relevant parts of the input in a very interpretable way. Can you share some insights on that?</speak>\"},\n",
       " {'speaker': 'Ashish Vaswani',\n",
       "  'speech': \"<speak>Yes, the attention visualizations are quite fascinating. <break time='1s'/> We found that different attention heads appear to specialize in different tasks like tracking long-range dependencies, performing anaphora resolution, and capturing syntactic structures. It's remarkable how the model learns these capabilities in an unsupervised way, simply by attending to the relevant parts of the input. We even see attention patterns that seem to correspond to linguistic concepts like subject-verb agreement.</speak>\"},\n",
       " {'speaker': 'Amy',\n",
       "  'speech': \"<speak>That's incredible! It really showcases the power and interpretability of attention-based models. <break time='1s'/> Before we wrap up Ashish, what are some future directions you're excited about for this line of research?</speak>\"},\n",
       " {'speaker': 'Ashish Vaswani',\n",
       "  'speech': \"<speak>There are many exciting avenues to explore. We plan to apply the Transformer to other modalities beyond text, like images, audio and video. <break time='1s'/> Making the generation process less sequential is another goal, which could lead to more efficient decoding. And of course, further improving the model's ability to capture long-range dependencies remains an important challenge. I'm confident attention-based architectures will continue to push the boundaries of what's possible in sequence modeling.</speak>\"},\n",
       " {'speaker': 'Amy',\n",
       "  'speech': '<speak>Those are really promising future directions. Thank you Ashish for sharing your insights on this pioneering work. To our listeners, I encourage you to dive deeper into the Transformer and its fascinating attention mechanisms. This could be the start of a new era in neural network architectures.</speak>'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bff10b38-36ee-4cbe-af39-87464caf49c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript = '\\n'.join(af['speech'] for af in extracted_speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf1203a5-76c6-4e59-84b8-a04731aabe97",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfiles = generate_audio_files(guest_name,extracted_speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1ee0073-c849-471a-9463-b8c5483d2d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "podcast_file = combine_files(title,sfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4477fea-4389-4e61-b4ac-3b567d42baf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'audio/Exploring the Transformer: A Revolutionary Attention-Based Model.mp3'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "podcast_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83472cb9-9191-4665-877e-c3167fa53945",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
