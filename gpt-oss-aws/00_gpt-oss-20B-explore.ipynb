{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8800a58f-46c7-4530-b33a-7102fb980be9",
   "metadata": {},
   "source": [
    "# GPT OSS 20B Model Exploration\n",
    "\n",
    "This notebook provides a comprehensive exploration of OpenAI's GPT OSS 20B model, focusing on its architecture, characteristics, and capabilities. We'll examine the model structure, understand its Mixture of Experts (MoE) architecture, and demonstrate the Harmony format with different reasoning levels.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **Model Architecture**: Deep dive into GPT OSS 20B's technical specifications\n",
    "2. **Parameter Analysis**: Understanding the model's scale and structure\n",
    "3. **Mixture of Experts**: How MoE architecture works in this model\n",
    "4. **Harmony Format**: OpenAI's reasoning format and different levels\n",
    "5. **SageMaker Integration**: Best practices for model exploration in SageMaker AI Studio\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- **Compute Requirements**: This notebook requires significant computational resources (GPU recommended)\n",
    "- **Environment**: Best run in Amazon SageMaker AI Studio Jupyter space with GPU instances\n",
    "- **Memory**: At least 16GB RAM, preferably 32GB+ for optimal performance\n",
    "- **Storage**: Sufficient space for model weights (~40GB)\n",
    "\n",
    "## SageMaker AI Studio Setup\n",
    "\n",
    "If you're running this in SageMaker AI Studio:\n",
    "\n",
    "1. **Instance Type**: Use `ml.g4dn.xlarge` or larger for GPU acceleration\n",
    "2. **Kernel**: Python 3 (PyTorch) kernel recommended\n",
    "3. **Permissions**: Ensure your execution role has access to download models from Hugging Face\n",
    "\n",
    "Let's start by installing the required dependencies and downloading the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ac6cb8-681a-45af-99b2-e34282a13705",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U transformers kernels accelerate torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff78b38-20fe-4dd8-8e62-3de60d7fce29",
   "metadata": {},
   "source": [
    "## Model Download and Setup\n",
    "\n",
    "We'll download the GPT OSS 20B model from Hugging Face. This model is available in different quantization levels:\n",
    "\n",
    "- **Full Precision**: ~80GB (float32)\n",
    "- **Half Precision**: ~40GB (float16/bfloat16) \n",
    "- **INT4 Quantized**: ~20GB (4-bit quantization)\n",
    "\n",
    "For this exploration, we'll use the INT4 quantized version for better memory efficiency while maintaining good performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e28897a-acd3-45dd-b845-8a2b6ea0fd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli download openai/gpt-oss-20b --local-dir gpt-oss-20b-int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52a62d9-0e19-4f50-a2ac-83a36e2154eb",
   "metadata": {},
   "source": [
    "## Model Loading and Initialization\n",
    "\n",
    "Now we'll load the model using the Transformers library. The `device_map=\"auto\"` parameter automatically distributes the model across available GPUs and CPU memory as needed.\n",
    "\n",
    "### Key Loading Parameters:\n",
    "- **`device_map=\"auto\"`**: Automatic device placement for optimal memory usage\n",
    "- **Model Path**: Points to our locally downloaded model\n",
    "- **Tokenizer**: Handles text encoding/decoding for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b15c1e-2bc7-41cd-b31f-4ba2987f485c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt-oss-20b-int4\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt-oss-20b-int4\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1351cef-79dc-43bd-b2d8-a1142db00c01",
   "metadata": {},
   "source": [
    "## Model Architecture Analysis\n",
    "\n",
    "Let's examine the GPT OSS 20B model architecture in detail. This model represents a significant advancement in open-source language models, featuring:\n",
    "\n",
    "### Key Architectural Features:\n",
    "\n",
    "1. **Mixture of Experts (MoE)**: Uses sparse activation to achieve high capacity with efficient inference\n",
    "2. **Transformer Architecture**: Based on the proven transformer decoder architecture\n",
    "3. **Attention Mechanism**: Multi-head attention with optimized key-value projections\n",
    "4. **Layer Normalization**: RMSNorm for improved training stability\n",
    "5. **Rotary Position Embedding**: Advanced positional encoding for better long-context understanding\n",
    "\n",
    "### Model Scale:\n",
    "- **Parameters**: ~20 billion parameters total\n",
    "- **Active Parameters**: ~2-3 billion per forward pass (due to MoE)\n",
    "- **Layers**: 24 transformer decoder layers\n",
    "- **Hidden Size**: 2880 dimensions\n",
    "- **Vocabulary**: 201,088 tokens\n",
    "\n",
    "Let's examine the model structure:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d353568d-7d25-4869-a22e-9cf0ffbaa204",
   "metadata": {},
   "source": [
    "### Model Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fd78877-072d-4724-84aa-9b690559f660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Model view\n",
      "--------------------------------------------------\n",
      "GptOssForCausalLM(\n",
      "  (model): GptOssModel(\n",
      "    (embed_tokens): Embedding(201088, 2880, padding_idx=199999)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x GptOssDecoderLayer(\n",
      "        (self_attn): GptOssAttention(\n",
      "          (q_proj): Linear(in_features=2880, out_features=4096, bias=True)\n",
      "          (k_proj): Linear(in_features=2880, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=2880, out_features=512, bias=True)\n",
      "          (o_proj): Linear(in_features=4096, out_features=2880, bias=True)\n",
      "        )\n",
      "        (mlp): GptOssMLP(\n",
      "          (router): GptOssTopKRouter()\n",
      "          (experts): Mxfp4GptOssExperts()\n",
      "        )\n",
      "        (input_layernorm): GptOssRMSNorm((2880,), eps=1e-05)\n",
      "        (post_attention_layernorm): GptOssRMSNorm((2880,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): GptOssRMSNorm((2880,), eps=1e-05)\n",
      "    (rotary_emb): GptOssRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2880, out_features=201088, bias=False)\n",
      ")\n",
      "--------------------------------------------------\n",
      "Layers\n",
      "--------------------------------------------------\n",
      "Number of Layers (Transformer blocks): 24\n",
      "GptOssDecoderLayer(\n",
      "  (self_attn): GptOssAttention(\n",
      "    (q_proj): Linear(in_features=2880, out_features=4096, bias=True)\n",
      "    (k_proj): Linear(in_features=2880, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=2880, out_features=512, bias=True)\n",
      "    (o_proj): Linear(in_features=4096, out_features=2880, bias=True)\n",
      "  )\n",
      "  (mlp): GptOssMLP(\n",
      "    (router): GptOssTopKRouter()\n",
      "    (experts): Mxfp4GptOssExperts()\n",
      "  )\n",
      "  (input_layernorm): GptOssRMSNorm((2880,), eps=1e-05)\n",
      "  (post_attention_layernorm): GptOssRMSNorm((2880,), eps=1e-05)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print('-'*50)\n",
    "print('Model view')\n",
    "print('-'*50)\n",
    "print(model)\n",
    "\n",
    "print('-'*50)\n",
    "print('Layers')\n",
    "print('-'*50)\n",
    "num_layers = len(model.model.layers)\n",
    "print(\"Number of Layers (Transformer blocks):\", num_layers)\n",
    "# Peek into one block\n",
    "layer_0 = model.model.layers[0]\n",
    "print(layer_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32029f06-fa0b-48cf-bb04-4c059a39db65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed_tokens.weight: torch.Size([201088, 2880]), torch.bfloat16\n",
      "layers.0.self_attn.sinks: torch.Size([64]), torch.bfloat16\n",
      "layers.0.self_attn.q_proj.weight: torch.Size([4096, 2880]), torch.bfloat16\n",
      "layers.0.self_attn.q_proj.bias: torch.Size([4096]), torch.bfloat16\n",
      "layers.0.self_attn.k_proj.weight: torch.Size([512, 2880]), torch.bfloat16\n",
      "layers.0.self_attn.k_proj.bias: torch.Size([512]), torch.bfloat16\n",
      "layers.0.self_attn.v_proj.weight: torch.Size([512, 2880]), torch.bfloat16\n",
      "layers.0.self_attn.v_proj.bias: torch.Size([512]), torch.bfloat16\n",
      "layers.0.self_attn.o_proj.weight: torch.Size([2880, 4096]), torch.bfloat16\n",
      "layers.0.self_attn.o_proj.bias: torch.Size([2880]), torch.bfloat16\n",
      "layers.0.mlp.router.weight: torch.Size([32, 2880]), torch.bfloat16\n",
      "layers.0.mlp.router.bias: torch.Size([32]), torch.bfloat16\n",
      "layers.0.mlp.experts.gate_up_proj_bias: torch.Size([32, 5760]), torch.bfloat16\n",
      "layers.0.mlp.experts.down_proj_bias: torch.Size([32, 2880]), torch.bfloat16\n",
      "layers.0.input_layernorm.weight: torch.Size([2880]), torch.bfloat16\n",
      "layers.0.post_attention_layernorm.weight: torch.Size([2880]), torch.bfloat16\n",
      "layers.1.self_attn.sinks: torch.Size([64]), torch.bfloat16\n",
      "layers.1.self_attn.q_proj.weight: torch.Size([4096, 2880]), torch.bfloat16\n",
      "layers.1.self_attn.q_proj.bias: torch.Size([4096]), torch.bfloat16\n",
      "layers.1.self_attn.k_proj.weight: torch.Size([512, 2880]), torch.bfloat16\n",
      "layers.1.self_attn.k_proj.bias: torch.Size([512]), torch.bfloat16\n",
      "layers.1.self_attn.v_proj.weight: torch.Size([512, 2880]), torch.bfloat16\n",
      "layers.1.self_attn.v_proj.bias: torch.Size([512]), torch.bfloat16\n",
      "layers.1.self_attn.o_proj.weight: torch.Size([2880, 4096]), torch.bfloat16\n",
      "layers.1.self_attn.o_proj.bias: torch.Size([2880]), torch.bfloat16\n",
      "layers.1.mlp.router.weight: torch.Size([32, 2880]), torch.bfloat16\n",
      "layers.1.mlp.router.bias: torch.Size([32]), torch.bfloat16\n",
      "layers.1.mlp.experts.gate_up_proj_bias: torch.Size([32, 5760]), torch.bfloat16\n",
      "layers.1.mlp.experts.down_proj_bias: torch.Size([32, 2880]), torch.bfloat16\n",
      "layers.1.input_layernorm.weight: torch.Size([2880]), torch.bfloat16\n",
      "layers.1.post_attention_layernorm.weight: torch.Size([2880]), torch.bfloat16\n",
      "layers.2.self_attn.sinks: torch.Size([64]), torch.bfloat16\n",
      "layers.2.self_attn.q_proj.weight: torch.Size([4096, 2880]), torch.bfloat16\n",
      "layers.2.self_attn.q_proj.bias: torch.Size([4096]), torch.bfloat16\n",
      "layers.2.self_attn.k_proj.weight: torch.Size([512, 2880]), torch.bfloat16\n",
      "layers.2.self_attn.k_proj.bias: torch.Size([512]), torch.bfloat16\n",
      "layers.2.self_attn.v_proj.weight: torch.Size([512, 2880]), torch.bfloat16\n",
      "layers.2.self_attn.v_proj.bias: torch.Size([512]), torch.bfloat16\n",
      "layers.2.self_attn.o_proj.weight: torch.Size([2880, 4096]), torch.bfloat16\n",
      "layers.2.self_attn.o_proj.bias: torch.Size([2880]), torch.bfloat16\n",
      "layers.2.mlp.router.weight: torch.Size([32, 2880]), torch.bfloat16\n",
      "layers.2.mlp.router.bias: torch.Size([32]), torch.bfloat16\n",
      "layers.2.mlp.experts.gate_up_proj_bias: torch.Size([32, 5760]), torch.bfloat16\n",
      "layers.2.mlp.experts.down_proj_bias: torch.Size([32, 2880]), torch.bfloat16\n",
      "layers.2.input_layernorm.weight: torch.Size([2880]), torch.bfloat16\n",
      "layers.2.post_attention_layernorm.weight: torch.Size([2880]), torch.bfloat16\n",
      "layers.3.self_attn.sinks: torch.Size([64]), torch.bfloat16\n",
      "layers.3.self_attn.q_proj.weight: torch.Size([4096, 2880]), torch.bfloat16\n",
      "layers.3.self_attn.q_proj.bias: torch.Size([4096]), torch.bfloat16\n",
      "layers.3.self_attn.k_proj.weight: torch.Size([512, 2880]), torch.bfloat16\n",
      "layers.3.self_attn.k_proj.bias: torch.Size([512]), torch.bfloat16\n",
      "layers.3.self_attn.v_proj.weight: torch.Size([512, 2880]), torch.bfloat16\n",
      "layers.3.self_attn.v_proj.bias: torch.Size([512]), torch.bfloat16\n",
      "layers.3.self_attn.o_proj.weight: torch.Size([2880, 4096]), torch.bfloat16\n",
      "layers.3.self_attn.o_proj.bias: torch.Size([2880]), torch.bfloat16\n",
      "layers.3.mlp.router.weight: torch.Size([32, 2880]), torch.bfloat16\n",
      "layers.3.mlp.router.bias: torch.Size([32]), torch.bfloat16\n",
      "layers.3.mlp.experts.gate_up_proj_bias: torch.Size([32, 5760]), torch.bfloat16\n",
      "layers.3.mlp.experts.down_proj_bias: torch.Size([32, 2880]), torch.bfloat16\n",
      "layers.3.input_layernorm.weight: torch.Size([2880]), torch.bfloat16\n",
      "layers.3.post_attention_layernorm.weight: torch.Size([2880]), torch.bfloat16\n",
      "layers.4.self_attn.sinks: torch.Size([64]), torch.bfloat16\n",
      "layers.4.self_attn.q_proj.weight: torch.Size([4096, 2880]), torch.bfloat16\n",
      "layers.4.self_attn.q_proj.bias: torch.Size([4096]), torch.bfloat16\n",
      "layers.4.self_attn.k_proj.weight: torch.Size([512, 2880]), torch.bfloat16\n",
      "layers.4.self_attn.k_proj.bias: torch.Size([512]), torch.bfloat16\n",
      "layers.4.self_attn.v_proj.weight: torch.Size([512, 2880]), torch.bfloat16\n",
      "layers.4.self_attn.v_proj.bias: torch.Size([512]), torch.bfloat16\n",
      "layers.4.self_attn.o_proj.weight: torch.Size([2880, 4096]), torch.bfloat16\n",
      "layers.4.self_attn.o_proj.bias: torch.Size([2880]), torch.bfloat16\n",
      "layers.4.mlp.router.weight: torch.Size([32, 2880]), torch.bfloat16\n",
      "layers.4.mlp.router.bias: torch.Size([32]), torch.bfloat16\n",
      "layers.4.mlp.experts.gate_up_proj_bias: torch.Size([32, 5760]), torch.bfloat16\n",
      "layers.4.mlp.experts.down_proj_bias: torch.Size([32, 2880]), torch.bfloat16\n",
      "layers.4.input_layernorm.weight: torch.Size([2880]), torch.bfloat16\n",
      "layers.4.post_attention_layernorm.weight: torch.Size([2880]), torch.bfloat16\n",
      "layers.5.self_attn.sinks: torch.Size([64]), torch.bfloat16\n",
      "layers.5.self_attn.q_proj.weight: torch.Size([4096, 2880]), torch.bfloat16\n",
      "layers.5.self_attn.q_proj.bias: torch.Size([4096]), torch.bfloat16\n",
      "layers.5.self_attn.k_proj.weight: torch.Size([512, 2880]), torch.bfloat16\n",
      "layers.5.self_attn.k_proj.bias: torch.Size([512]), torch.bfloat16\n",
      "layers.5.self_attn.v_proj.weight: torch.Size([512, 2880]), torch.bfloat16\n",
      "layers.5.self_attn.v_proj.bias: torch.Size([512]), torch.bfloat16\n",
      "layers.5.self_attn.o_proj.weight: torch.Size([2880, 4096]), torch.bfloat16\n",
      "layers.5.self_attn.o_proj.bias: torch.Size([2880]), torch.bfloat16\n",
      "layers.5.mlp.router.weight: torch.Size([32, 2880]), torch.bfloat16\n",
      "layers.5.mlp.router.bias: torch.Size([32]), torch.bfloat16\n",
      "layers.5.mlp.experts.gate_up_proj_bias: torch.Size([32, 5760]), torch.bfloat16\n",
      "layers.5.mlp.experts.down_proj_bias: torch.Size([32, 2880]), torch.bfloat16\n",
      "layers.5.input_layernorm.weight: torch.Size([2880]), torch.bfloat16\n",
      "layers.5.post_attention_layernorm.weight: torch.Size([2880]), torch.bfloat16\n",
      "layers.6.self_attn.sinks: torch.Size([64]), torch.bfloat16\n",
      "layers.6.self_attn.q_proj.weight: torch.Size([4096, 2880]), torch.bfloat16\n",
      "layers.6.self_attn.q_proj.bias: torch.Size([4096]), torch.bfloat16\n",
      "layers.6.self_attn.k_proj.weight: torch.Size([512, 2880]), torch.bfloat16\n",
      "layers.6.self_attn.k_proj.bias: torch.Size([512]), torch.bfloat16\n",
      "layers.6.self_attn.v_proj.weight: torch.Size([512, 2880]), torch.bfloat16\n",
      "layers.6.self_attn.v_proj.bias: torch.Size([512]), torch.bfloat16\n",
      "layers.6.self_attn.o_proj.weight: torch.Size([2880, 4096]), torch.bfloat16\n",
      "layers.6.self_attn.o_proj.bias: torch.Size([2880]), torch.bfloat16\n",
      "layers.6.mlp.router.weight: torch.Size([32, 2880]), torch.bfloat16\n",
      "layers.6.mlp.router.bias: torch.Size([32]), torch.bfloat16\n",
      "layers.6.mlp.experts.gate_up_proj_bias: torch.Size([32, 5760]), torch.bfloat16\n",
      "layers.6.mlp.experts.down_proj_bias: torch.Size([32, 2880]), torch.bfloat16\n",
      "layers.6.input_layernorm.weight: torch.Size([2880]), torch.bfloat16\n",
      "layers.6.post_attention_layernorm.weight: torch.Size([2880]), torch.bfloat16\n",
      "layers.7.self_attn.sinks: torch.Size([64]), torch.bfloat16\n",
      "layers.7.self_attn.q_proj.weight: torch.Size([4096, 2880]), torch.bfloat16\n",
      "layers.7.self_attn.q_proj.bias: torch.Size([4096]), torch.bfloat16\n",
      "layers.7.self_attn.k_proj.weight: torch.Size([512, 2880]), torch.bfloat16\n",
      "layers.7.self_attn.k_proj.bias: torch.Size([512]), torch.bfloat16\n",
      "layers.7.self_attn.v_proj.weight: torch.Size([512, 2880]), torch.bfloat16\n",
      "layers.7.self_attn.v_proj.bias: torch.Size([512]), torch.bfloat16\n",
      "layers.7.self_attn.o_proj.weight: torch.Size([2880, 4096]), torch.bfloat16\n",
      "layers.7.self_attn.o_proj.bias: torch.Size([2880]), torch.bfloat16\n",
      "layers.7.mlp.router.weight: torch.Size([32, 2880]), torch.bfloat16\n",
      "layers.7.mlp.router.bias: torch.Size([32]), torch.bfloat16\n",
      "layers.7.mlp.experts.gate_up_proj_bias: torch.Size([32, 5760]), torch.bfloat16\n",
      "layers.7.mlp.experts.down_proj_bias: torch.Size([32, 2880]), torch.bfloat16\n",
      "layers.7.input_layernorm.weight: torch.Size([2880]), torch.bfloat16\n",
      "layers.7.post_attention_layernorm.weight: torch.Size([2880]), torch.bfloat16\n",
      "layers.8.self_attn.sinks: torch.Size([64]), torch.bfloat16\n",
      "layers.8.self_attn.q_proj.weight: torch.Size([4096, 2880]), torch.bfloat16\n",
      "layers.8.self_attn.q_proj.bias: torch.Size([4096]), torch.bfloat16\n",
      "layers.8.self_attn.k_proj.weight: torch.Size([512, 2880]), torch.bfloat16\n",
      "layers.8.self_attn.k_proj.bias: torch.Size([512]), torch.bfloat16\n",
      "layers.8.self_attn.v_proj.weight: torch.Size([512, 2880]), torch.bfloat16\n",
      "layers.8.self_attn.v_proj.bias: torch.Size([512]), torch.bfloat16\n",
      "layers.8.self_attn.o_proj.weight: torch.Size([2880, 4096]), torch.bfloat16\n",
      "layers.8.self_attn.o_proj.bias: torch.Size([2880]), torch.bfloat16\n",
      "layers.8.mlp.router.weight: torch.Size([32, 2880]), torch.bfloat16\n",
      "layers.8.mlp.router.bias: torch.Size([32]), torch.bfloat16\n",
      "layers.8.mlp.experts.gate_up_proj_bias: torch.Size([32, 5760]), torch.bfloat16\n",
      "layers.8.mlp.experts.down_proj_bias: torch.Size([32, 2880]), torch.bfloat16\n",
      "layers.8.input_layernorm.weight: torch.Size([2880]), torch.bfloat16\n",
      "layers.8.post_attention_layernorm.weight: torch.Size([2880]), torch.bfloat16\n",
      "layers.9.self_attn.sinks: torch.Size([64]), torch.bfloat16\n",
      "layers.9.self_attn.q_proj.weight: torch.Size([4096, 2880]), torch.bfloat16\n",
      "layers.9.self_attn.q_proj.bias: torch.Size([4096]), torch.bfloat16\n",
      "layers.9.self_attn.k_proj.weight: torch.Size([512, 2880]), torch.bfloat16\n",
      "layers.9.self_attn.k_proj.bias: torch.Size([512]), torch.bfloat16\n",
      "layers.9.self_attn.v_proj.weight: torch.Size([512, 2880]), torch.bfloat16\n",
      "layers.9.self_attn.v_proj.bias: torch.Size([512]), torch.bfloat16\n",
      "layers.9.self_attn.o_proj.weight: torch.Size([2880, 4096]), torch.bfloat16\n",
      "layers.9.self_attn.o_proj.bias: torch.Size([2880]), torch.bfloat16\n",
      "layers.9.mlp.router.weight: torch.Size([32, 2880]), torch.bfloat16\n",
      "layers.9.mlp.router.bias: torch.Size([32]), torch.bfloat16\n",
      "layers.9.mlp.experts.gate_up_proj_bias: torch.Size([32, 5760]), torch.bfloat16\n",
      "layers.9.mlp.experts.down_proj_bias: torch.Size([32, 2880]), torch.bfloat16\n",
      "layers.9.input_layernorm.weight: torch.Size([2880]), torch.bfloat16\n",
      "layers.9.post_attention_layernorm.weight: torch.Size([2880]), torch.bfloat16\n",
      "layers.10.self_attn.sinks: torch.Size([64]), torch.bfloat16\n",
      "layers.10.self_attn.q_proj.weight: torch.Size([4096, 2880]), torch.bfloat16\n",
      "layers.10.self_attn.q_proj.bias: torch.Size([4096]), torch.bfloat16\n",
      "layers.10.self_attn.k_proj.weight: torch.Size([512, 2880]), torch.bfloat16\n",
      "layers.10.self_attn.k_proj.bias: torch.Size([512]), torch.bfloat16\n",
      "layers.10.self_attn.v_proj.weight: torch.Size([512, 2880]), torch.bfloat16\n",
      "layers.10.self_attn.v_proj.bias: torch.Size([512]), torch.bfloat16\n",
      "layers.10.self_attn.o_proj.weight: torch.Size([2880, 4096]), torch.bfloat16\n",
      "layers.10.self_attn.o_proj.bias: torch.Size([2880]), torch.bfloat16\n",
      "layers.10.mlp.router.weight: torch.Size([32, 2880]), torch.bfloat16\n",
      "layers.10.mlp.router.bias: torch.Size([32]), torch.bfloat16\n",
      "layers.10.mlp.experts.gate_up_proj_bias: torch.Size([32, 5760]), torch.bfloat16\n",
      "layers.10.mlp.experts.down_proj_bias: torch.Size([32, 2880]), torch.bfloat16\n",
      "layers.10.input_layernorm.weight: torch.Size([2880]), torch.bfloat16\n",
      "layers.10.post_attention_layernorm.weight: torch.Size([2880]), torch.bfloat16\n",
      "layers.11.self_attn.sinks: torch.Size([64]), torch.bfloat16\n",
      "layers.11.self_attn.q_proj.weight: torch.Size([4096, 2880]), torch.bfloat16\n",
      "layers.11.self_attn.q_proj.bias: torch.Size([4096]), torch.bfloat16\n",
      "layers.11.self_attn.k_proj.weight: torch.Size([512, 2880]), torch.bfloat16\n",
      "layers.11.self_attn.k_proj.bias: torch.Size([512]), torch.bfloat16\n",
      "layers.11.self_attn.v_proj.weight: torch.Size([512, 2880]), torch.bfloat16\n",
      "layers.11.self_attn.v_proj.bias: torch.Size([512]), torch.bfloat16\n",
      "layers.11.self_attn.o_proj.weight: torch.Size([2880, 4096]), torch.bfloat16\n",
      "layers.11.self_attn.o_proj.bias: torch.Size([2880]), torch.bfloat16\n",
      "layers.11.mlp.router.weight: torch.Size([32, 2880]), torch.bfloat16\n",
      "layers.11.mlp.router.bias: torch.Size([32]), torch.bfloat16\n",
      "layers.11.mlp.experts.gate_up_proj_bias: torch.Size([32, 5760]), torch.bfloat16\n",
      "layers.11.mlp.experts.down_proj_bias: torch.Size([32, 2880]), torch.bfloat16\n",
      "layers.11.input_layernorm.weight: torch.Size([2880]), torch.bfloat16\n",
      "layers.11.post_attention_layernorm.weight: torch.Size([2880]), torch.bfloat16\n",
      "layers.12.self_attn.sinks: torch.Size([64]), torch.bfloat16\n",
      "layers.12.self_attn.q_proj.weight: torch.Size([4096, 2880]), torch.bfloat16\n",
      "layers.12.self_attn.q_proj.bias: torch.Size([4096]), torch.bfloat16\n",
      "layers.12.self_attn.k_proj.weight: torch.Size([512, 2880]), torch.bfloat16\n",
      "layers.12.self_attn.k_proj.bias: torch.Size([512]), torch.bfloat16\n",
      "layers.12.self_attn.v_proj.weight: torch.Size([512, 2880]), torch.bfloat16\n",
      "layers.12.self_attn.v_proj.bias: torch.Size([512]), torch.bfloat16\n",
      "layers.12.self_attn.o_proj.weight: torch.Size([2880, 4096]), torch.bfloat16\n",
      "layers.12.self_attn.o_proj.bias: torch.Size([2880]), torch.bfloat16\n",
      "layers.12.mlp.router.weight: torch.Size([32, 2880]), torch.bfloat16\n",
      "layers.12.mlp.router.bias: torch.Size([32]), torch.bfloat16\n",
      "layers.12.mlp.experts.gate_up_proj_bias: torch.Size([32, 5760]), torch.bfloat16\n",
      "layers.12.mlp.experts.down_proj_bias: torch.Size([32, 2880]), torch.bfloat16\n",
      "layers.12.input_layernorm.weight: torch.Size([2880]), torch.bfloat16\n",
      "layers.12.post_attention_layernorm.weight: torch.Size([2880]), torch.bfloat16\n",
      "layers.13.self_attn.sinks: torch.Size([64]), torch.bfloat16\n",
      "layers.13.self_attn.q_proj.weight: torch.Size([4096, 2880]), torch.bfloat16\n",
      "layers.13.self_attn.q_proj.bias: torch.Size([4096]), torch.bfloat16\n",
      "layers.13.self_attn.k_proj.weight: torch.Size([512, 2880]), torch.bfloat16\n",
      "layers.13.self_attn.k_proj.bias: torch.Size([512]), torch.bfloat16\n",
      "layers.13.self_attn.v_proj.weight: torch.Size([512, 2880]), torch.bfloat16\n",
      "layers.13.self_attn.v_proj.bias: torch.Size([512]), torch.bfloat16\n",
      "layers.13.self_attn.o_proj.weight: torch.Size([2880, 4096]), torch.bfloat16\n",
      "layers.13.self_attn.o_proj.bias: torch.Size([2880]), torch.bfloat16\n",
      "layers.13.mlp.router.weight: torch.Size([32, 2880]), torch.bfloat16\n",
      "layers.13.mlp.router.bias: torch.Size([32]), torch.bfloat16\n",
      "layers.13.mlp.experts.gate_up_proj_bias: torch.Size([32, 5760]), torch.bfloat16\n",
      "layers.13.mlp.experts.down_proj_bias: torch.Size([32, 2880]), torch.bfloat16\n",
      "layers.13.input_layernorm.weight: torch.Size([2880]), torch.bfloat16\n",
      "layers.13.post_attention_layernorm.weight: torch.Size([2880]), torch.bfloat16\n",
      "layers.14.self_attn.sinks: torch.Size([64]), torch.bfloat16\n",
      "layers.14.self_attn.q_proj.weight: torch.Size([4096, 2880]), torch.bfloat16\n",
      "layers.14.self_attn.q_proj.bias: torch.Size([4096]), torch.bfloat16\n",
      "layers.14.self_attn.k_proj.weight: torch.Size([512, 2880]), torch.bfloat16\n",
      "layers.14.self_attn.k_proj.bias: torch.Size([512]), torch.bfloat16\n",
      "layers.14.self_attn.v_proj.weight: torch.Size([512, 2880]), torch.bfloat16\n",
      "layers.14.self_attn.v_proj.bias: torch.Size([512]), torch.bfloat16\n",
      "layers.14.self_attn.o_proj.weight: torch.Size([2880, 4096]), torch.bfloat16\n",
      "layers.14.self_attn.o_proj.bias: torch.Size([2880]), torch.bfloat16\n",
      "layers.14.mlp.router.weight: torch.Size([32, 2880]), torch.bfloat16\n",
      "layers.14.mlp.router.bias: torch.Size([32]), torch.bfloat16\n",
      "layers.14.mlp.experts.gate_up_proj_bias: torch.Size([32, 5760]), torch.bfloat16\n",
      "layers.14.mlp.experts.down_proj_bias: torch.Size([32, 2880]), torch.bfloat16\n",
      "layers.14.input_layernorm.weight: torch.Size([2880]), torch.bfloat16\n",
      "layers.14.post_attention_layernorm.weight: torch.Size([2880]), torch.bfloat16\n",
      "layers.15.self_attn.sinks: torch.Size([64]), torch.bfloat16\n",
      "layers.15.self_attn.q_proj.weight: torch.Size([4096, 2880]), torch.bfloat16\n",
      "layers.15.self_attn.q_proj.bias: torch.Size([4096]), torch.bfloat16\n",
      "layers.15.self_attn.k_proj.weight: torch.Size([512, 2880]), torch.bfloat16\n",
      "layers.15.self_attn.k_proj.bias: torch.Size([512]), torch.bfloat16\n",
      "layers.15.self_attn.v_proj.weight: torch.Size([512, 2880]), torch.bfloat16\n",
      "layers.15.self_attn.v_proj.bias: torch.Size([512]), torch.bfloat16\n",
      "layers.15.self_attn.o_proj.weight: torch.Size([2880, 4096]), torch.bfloat16\n",
      "layers.15.self_attn.o_proj.bias: torch.Size([2880]), torch.bfloat16\n",
      "layers.15.mlp.router.weight: torch.Size([32, 2880]), torch.bfloat16\n",
      "layers.15.mlp.router.bias: torch.Size([32]), torch.bfloat16\n",
      "layers.15.mlp.experts.gate_up_proj_bias: torch.Size([32, 5760]), torch.bfloat16\n",
      "layers.15.mlp.experts.down_proj_bias: torch.Size([32, 2880]), torch.bfloat16\n",
      "layers.15.input_layernorm.weight: torch.Size([2880]), torch.bfloat16\n",
      "layers.15.post_attention_layernorm.weight: torch.Size([2880]), torch.bfloat16\n",
      "layers.16.self_attn.sinks: torch.Size([64]), torch.bfloat16\n",
      "layers.16.self_attn.q_proj.weight: torch.Size([4096, 2880]), torch.bfloat16\n",
      "layers.16.self_attn.q_proj.bias: torch.Size([4096]), torch.bfloat16\n",
      "layers.16.self_attn.k_proj.weight: torch.Size([512, 2880]), torch.bfloat16\n",
      "layers.16.self_attn.k_proj.bias: torch.Size([512]), torch.bfloat16\n",
      "layers.16.self_attn.v_proj.weight: torch.Size([512, 2880]), torch.bfloat16\n",
      "layers.16.self_attn.v_proj.bias: torch.Size([512]), torch.bfloat16\n",
      "layers.16.self_attn.o_proj.weight: torch.Size([2880, 4096]), torch.bfloat16\n",
      "layers.16.self_attn.o_proj.bias: torch.Size([2880]), torch.bfloat16\n",
      "layers.16.mlp.router.weight: torch.Size([32, 2880]), torch.bfloat16\n",
      "layers.16.mlp.router.bias: torch.Size([32]), torch.bfloat16\n",
      "layers.16.mlp.experts.gate_up_proj_bias: torch.Size([32, 5760]), torch.bfloat16\n",
      "layers.16.mlp.experts.down_proj_bias: torch.Size([32, 2880]), torch.bfloat16\n",
      "layers.16.input_layernorm.weight: torch.Size([2880]), torch.bfloat16\n",
      "layers.16.post_attention_layernorm.weight: torch.Size([2880]), torch.bfloat16\n",
      "layers.17.self_attn.sinks: torch.Size([64]), torch.bfloat16\n",
      "layers.17.self_attn.q_proj.weight: torch.Size([4096, 2880]), torch.bfloat16\n",
      "layers.17.self_attn.q_proj.bias: torch.Size([4096]), torch.bfloat16\n",
      "layers.17.self_attn.k_proj.weight: torch.Size([512, 2880]), torch.bfloat16\n",
      "layers.17.self_attn.k_proj.bias: torch.Size([512]), torch.bfloat16\n",
      "layers.17.self_attn.v_proj.weight: torch.Size([512, 2880]), torch.bfloat16\n",
      "layers.17.self_attn.v_proj.bias: torch.Size([512]), torch.bfloat16\n",
      "layers.17.self_attn.o_proj.weight: torch.Size([2880, 4096]), torch.bfloat16\n",
      "layers.17.self_attn.o_proj.bias: torch.Size([2880]), torch.bfloat16\n",
      "layers.17.mlp.router.weight: torch.Size([32, 2880]), torch.bfloat16\n",
      "layers.17.mlp.router.bias: torch.Size([32]), torch.bfloat16\n",
      "layers.17.mlp.experts.gate_up_proj_bias: torch.Size([32, 5760]), torch.bfloat16\n",
      "layers.17.mlp.experts.down_proj_bias: torch.Size([32, 2880]), torch.bfloat16\n",
      "layers.17.input_layernorm.weight: torch.Size([2880]), torch.bfloat16\n",
      "layers.17.post_attention_layernorm.weight: torch.Size([2880]), torch.bfloat16\n",
      "layers.18.self_attn.sinks: torch.Size([64]), torch.bfloat16\n",
      "layers.18.self_attn.q_proj.weight: torch.Size([4096, 2880]), torch.bfloat16\n",
      "layers.18.self_attn.q_proj.bias: torch.Size([4096]), torch.bfloat16\n",
      "layers.18.self_attn.k_proj.weight: torch.Size([512, 2880]), torch.bfloat16\n",
      "layers.18.self_attn.k_proj.bias: torch.Size([512]), torch.bfloat16\n",
      "layers.18.self_attn.v_proj.weight: torch.Size([512, 2880]), torch.bfloat16\n",
      "layers.18.self_attn.v_proj.bias: torch.Size([512]), torch.bfloat16\n",
      "layers.18.self_attn.o_proj.weight: torch.Size([2880, 4096]), torch.bfloat16\n",
      "layers.18.self_attn.o_proj.bias: torch.Size([2880]), torch.bfloat16\n",
      "layers.18.mlp.router.weight: torch.Size([32, 2880]), torch.bfloat16\n",
      "layers.18.mlp.router.bias: torch.Size([32]), torch.bfloat16\n",
      "layers.18.mlp.experts.gate_up_proj_bias: torch.Size([32, 5760]), torch.bfloat16\n",
      "layers.18.mlp.experts.down_proj_bias: torch.Size([32, 2880]), torch.bfloat16\n",
      "layers.18.input_layernorm.weight: torch.Size([2880]), torch.bfloat16\n",
      "layers.18.post_attention_layernorm.weight: torch.Size([2880]), torch.bfloat16\n",
      "layers.19.self_attn.sinks: torch.Size([64]), torch.bfloat16\n",
      "layers.19.self_attn.q_proj.weight: torch.Size([4096, 2880]), torch.bfloat16\n",
      "layers.19.self_attn.q_proj.bias: torch.Size([4096]), torch.bfloat16\n",
      "layers.19.self_attn.k_proj.weight: torch.Size([512, 2880]), torch.bfloat16\n",
      "layers.19.self_attn.k_proj.bias: torch.Size([512]), torch.bfloat16\n",
      "layers.19.self_attn.v_proj.weight: torch.Size([512, 2880]), torch.bfloat16\n",
      "layers.19.self_attn.v_proj.bias: torch.Size([512]), torch.bfloat16\n",
      "layers.19.self_attn.o_proj.weight: torch.Size([2880, 4096]), torch.bfloat16\n",
      "layers.19.self_attn.o_proj.bias: torch.Size([2880]), torch.bfloat16\n",
      "layers.19.mlp.router.weight: torch.Size([32, 2880]), torch.bfloat16\n",
      "layers.19.mlp.router.bias: torch.Size([32]), torch.bfloat16\n",
      "layers.19.mlp.experts.gate_up_proj_bias: torch.Size([32, 5760]), torch.bfloat16\n",
      "layers.19.mlp.experts.down_proj_bias: torch.Size([32, 2880]), torch.bfloat16\n",
      "layers.19.input_layernorm.weight: torch.Size([2880]), torch.bfloat16\n",
      "layers.19.post_attention_layernorm.weight: torch.Size([2880]), torch.bfloat16\n",
      "layers.20.self_attn.sinks: torch.Size([64]), torch.bfloat16\n",
      "layers.20.self_attn.q_proj.weight: torch.Size([4096, 2880]), torch.bfloat16\n",
      "layers.20.self_attn.q_proj.bias: torch.Size([4096]), torch.bfloat16\n",
      "layers.20.self_attn.k_proj.weight: torch.Size([512, 2880]), torch.bfloat16\n",
      "layers.20.self_attn.k_proj.bias: torch.Size([512]), torch.bfloat16\n",
      "layers.20.self_attn.v_proj.weight: torch.Size([512, 2880]), torch.bfloat16\n",
      "layers.20.self_attn.v_proj.bias: torch.Size([512]), torch.bfloat16\n",
      "layers.20.self_attn.o_proj.weight: torch.Size([2880, 4096]), torch.bfloat16\n",
      "layers.20.self_attn.o_proj.bias: torch.Size([2880]), torch.bfloat16\n",
      "layers.20.mlp.router.weight: torch.Size([32, 2880]), torch.bfloat16\n",
      "layers.20.mlp.router.bias: torch.Size([32]), torch.bfloat16\n",
      "layers.20.mlp.experts.gate_up_proj_bias: torch.Size([32, 5760]), torch.bfloat16\n",
      "layers.20.mlp.experts.down_proj_bias: torch.Size([32, 2880]), torch.bfloat16\n",
      "layers.20.input_layernorm.weight: torch.Size([2880]), torch.bfloat16\n",
      "layers.20.post_attention_layernorm.weight: torch.Size([2880]), torch.bfloat16\n",
      "layers.21.self_attn.sinks: torch.Size([64]), torch.bfloat16\n",
      "layers.21.self_attn.q_proj.weight: torch.Size([4096, 2880]), torch.bfloat16\n",
      "layers.21.self_attn.q_proj.bias: torch.Size([4096]), torch.bfloat16\n",
      "layers.21.self_attn.k_proj.weight: torch.Size([512, 2880]), torch.bfloat16\n",
      "layers.21.self_attn.k_proj.bias: torch.Size([512]), torch.bfloat16\n",
      "layers.21.self_attn.v_proj.weight: torch.Size([512, 2880]), torch.bfloat16\n",
      "layers.21.self_attn.v_proj.bias: torch.Size([512]), torch.bfloat16\n",
      "layers.21.self_attn.o_proj.weight: torch.Size([2880, 4096]), torch.bfloat16\n",
      "layers.21.self_attn.o_proj.bias: torch.Size([2880]), torch.bfloat16\n",
      "layers.21.mlp.router.weight: torch.Size([32, 2880]), torch.bfloat16\n",
      "layers.21.mlp.router.bias: torch.Size([32]), torch.bfloat16\n",
      "layers.21.mlp.experts.gate_up_proj_bias: torch.Size([32, 5760]), torch.bfloat16\n",
      "layers.21.mlp.experts.down_proj_bias: torch.Size([32, 2880]), torch.bfloat16\n",
      "layers.21.input_layernorm.weight: torch.Size([2880]), torch.bfloat16\n",
      "layers.21.post_attention_layernorm.weight: torch.Size([2880]), torch.bfloat16\n",
      "layers.22.self_attn.sinks: torch.Size([64]), torch.bfloat16\n",
      "layers.22.self_attn.q_proj.weight: torch.Size([4096, 2880]), torch.bfloat16\n",
      "layers.22.self_attn.q_proj.bias: torch.Size([4096]), torch.bfloat16\n",
      "layers.22.self_attn.k_proj.weight: torch.Size([512, 2880]), torch.bfloat16\n",
      "layers.22.self_attn.k_proj.bias: torch.Size([512]), torch.bfloat16\n",
      "layers.22.self_attn.v_proj.weight: torch.Size([512, 2880]), torch.bfloat16\n",
      "layers.22.self_attn.v_proj.bias: torch.Size([512]), torch.bfloat16\n",
      "layers.22.self_attn.o_proj.weight: torch.Size([2880, 4096]), torch.bfloat16\n",
      "layers.22.self_attn.o_proj.bias: torch.Size([2880]), torch.bfloat16\n",
      "layers.22.mlp.router.weight: torch.Size([32, 2880]), torch.bfloat16\n",
      "layers.22.mlp.router.bias: torch.Size([32]), torch.bfloat16\n",
      "layers.22.mlp.experts.gate_up_proj_bias: torch.Size([32, 5760]), torch.bfloat16\n",
      "layers.22.mlp.experts.down_proj_bias: torch.Size([32, 2880]), torch.bfloat16\n",
      "layers.22.input_layernorm.weight: torch.Size([2880]), torch.bfloat16\n",
      "layers.22.post_attention_layernorm.weight: torch.Size([2880]), torch.bfloat16\n",
      "layers.23.self_attn.sinks: torch.Size([64]), torch.bfloat16\n",
      "layers.23.self_attn.q_proj.weight: torch.Size([4096, 2880]), torch.bfloat16\n",
      "layers.23.self_attn.q_proj.bias: torch.Size([4096]), torch.bfloat16\n",
      "layers.23.self_attn.k_proj.weight: torch.Size([512, 2880]), torch.bfloat16\n",
      "layers.23.self_attn.k_proj.bias: torch.Size([512]), torch.bfloat16\n",
      "layers.23.self_attn.v_proj.weight: torch.Size([512, 2880]), torch.bfloat16\n",
      "layers.23.self_attn.v_proj.bias: torch.Size([512]), torch.bfloat16\n",
      "layers.23.self_attn.o_proj.weight: torch.Size([2880, 4096]), torch.bfloat16\n",
      "layers.23.self_attn.o_proj.bias: torch.Size([2880]), torch.bfloat16\n",
      "layers.23.mlp.router.weight: torch.Size([32, 2880]), torch.bfloat16\n",
      "layers.23.mlp.router.bias: torch.Size([32]), torch.bfloat16\n",
      "layers.23.mlp.experts.gate_up_proj_bias: torch.Size([32, 5760]), torch.bfloat16\n",
      "layers.23.mlp.experts.down_proj_bias: torch.Size([32, 2880]), torch.bfloat16\n",
      "layers.23.input_layernorm.weight: torch.Size([2880]), torch.bfloat16\n",
      "layers.23.post_attention_layernorm.weight: torch.Size([2880]), torch.bfloat16\n",
      "norm.weight: torch.Size([2880]), torch.bfloat16\n",
      "Model parameter types: {torch.bfloat16}\n"
     ]
    }
   ],
   "source": [
    "# Print dtype of each parameter\n",
    "dtypes = set()\n",
    "for name, param in model.model.named_parameters():\n",
    "    dtypes.add(param.dtype)\n",
    "    print(f\"{name}: {param.shape}, {param.dtype}\")\n",
    "\n",
    "print(\"Model parameter types:\", dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7113d3a4-ed31-48a0-8ccd-fb35bf59b9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: 32 experts\n",
      "Layer 1: 32 experts\n",
      "Layer 2: 32 experts\n",
      "Layer 3: 32 experts\n",
      "Layer 4: 32 experts\n",
      "Layer 5: 32 experts\n",
      "Layer 6: 32 experts\n",
      "Layer 7: 32 experts\n",
      "Layer 8: 32 experts\n",
      "Layer 9: 32 experts\n",
      "Layer 10: 32 experts\n",
      "Layer 11: 32 experts\n",
      "Layer 12: 32 experts\n",
      "Layer 13: 32 experts\n",
      "Layer 14: 32 experts\n",
      "Layer 15: 32 experts\n",
      "Layer 16: 32 experts\n",
      "Layer 17: 32 experts\n",
      "Layer 18: 32 experts\n",
      "Layer 19: 32 experts\n",
      "Layer 20: 32 experts\n",
      "Layer 21: 32 experts\n",
      "Layer 22: 32 experts\n",
      "Layer 23: 32 experts\n"
     ]
    }
   ],
   "source": [
    "expert_counts = []\n",
    "for i, layer in enumerate(model.model.layers):\n",
    "    count = layer.mlp.experts.gate_up_proj_bias.shape[0]\n",
    "    expert_counts.append(count)\n",
    "    print(f\"Layer {i}: {count} experts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21762c0b-c817-4e0c-9647-310e83afc99d",
   "metadata": {},
   "source": [
    "## Understanding the Attention Mechanism\n",
    "\n",
    "The attention mechanism in GPT OSS 20B uses an optimized design:\n",
    "\n",
    "### Attention Architecture Details:\n",
    "\n",
    "- **Query Projection**: 2880 → 4096 \n",
    "- **Key/Value Projections**: 2880 → 512 \n",
    "- **Output Projection**: 4096 → 2880 \n",
    "\n",
    "### Mixture of Experts (MoE) in the MLP\n",
    "\n",
    "The `GptOssMLP` contains:\n",
    "- **Router**: Determines which experts to activate for each token\n",
    "- **Experts**: Multiple specialized feed-forward networks\n",
    "- **Top-K Selection**: Only activates the most relevant experts per token\n",
    "\n",
    "This allows the model to have 20B total parameters while only using ~2-3B per forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67f98a1f-c161-4658-95b8-399124f3bb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.output_attentions = True\n",
    "inputs = tokenizer(\"How do I diagnose engine issues?\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs, max_new_tokens=128)\n",
    "model.config.output_attentions = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb9ac33d-604a-4903-93be-64ed55f597fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# outputs.attentions is a list: one entry per layer, each: (batch_size, num_heads, seq_len, seq_len)\n",
    "layer_attns = outputs.attentions[-1][0]  # Last layer, first batch\n",
    "head_sum = layer_attns.sum(dim=(1,2))    # Sum over tokens for each head\n",
    "\n",
    "# Visualize\n",
    "import matplotlib.pyplot as plt\n",
    "plt.bar(range(len(head_sum)), head_sum.detach().to(torch.float).cpu().numpy())\n",
    "plt.xlabel(\"Attention Head\")\n",
    "plt.ylabel(\"Total Activation\")\n",
    "plt.title(\"Attention Head Activation (last layer)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e04ac6d5-471d-42ec-9ce4-a32221a1bb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.42 GB allocated\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(round(torch.cuda.memory_allocated() / (1024 ** 3),2), \"GB allocated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4dc2dbb2-72a2-4b75-ac15-497e11926012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 1,804,459,584\n",
      "Trainable parameters: 1,797,824,064\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "903ee884-d2de-4f9b-be3f-6587433bc76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GptOssConfig {\n",
      "  \"architectures\": [\n",
      "    \"GptOssForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"eos_token_id\": 200002,\n",
      "  \"experts_per_token\": 4,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2880,\n",
      "  \"initial_context_length\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2880,\n",
      "  \"layer_types\": [\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\"\n",
      "  ],\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"gpt_oss\",\n",
      "  \"num_attention_heads\": 64,\n",
      "  \"num_experts_per_tok\": 4,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"num_local_experts\": 32,\n",
      "  \"output_router_logits\": false,\n",
      "  \"pad_token_id\": 199999,\n",
      "  \"quantization_config\": {\n",
      "    \"dequantize\": false,\n",
      "    \"modules_to_not_convert\": [\n",
      "      \"model.layers.*.self_attn\",\n",
      "      \"model.layers.*.mlp.router\",\n",
      "      \"model.embed_tokens\",\n",
      "      \"lm_head\"\n",
      "    ],\n",
      "    \"quant_method\": \"mxfp4\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"beta_fast\": 32.0,\n",
      "    \"beta_slow\": 1.0,\n",
      "    \"factor\": 32.0,\n",
      "    \"original_max_position_embeddings\": 4096,\n",
      "    \"rope_type\": \"yarn\",\n",
      "    \"truncate\": false\n",
      "  },\n",
      "  \"rope_theta\": 150000,\n",
      "  \"router_aux_loss_coef\": 0.9,\n",
      "  \"sliding_window\": 128,\n",
      "  \"swiglu_limit\": 7.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.55.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 201088\n",
      "}\n",
      "\n",
      "context length : 131072\n",
      "vocab size : 201088\n",
      "embedding dim : 2880\n",
      "total layers : 24\n",
      "head size : 64\n",
      "total key value heads : 8\n",
      "total attention heads : 64\n",
      "key shape : torch.Size([512, 2880])\n",
      "query shape : torch.Size([4096, 2880])\n"
     ]
    }
   ],
   "source": [
    "config = model.config\n",
    "print(config)\n",
    "\n",
    "vocab_size = getattr(config,'vocab_size')\n",
    "head_dim = getattr(config,'head_dim')\n",
    "key_heads = getattr(config,'num_key_value_heads')\n",
    "attn_heads = getattr(config,'num_attention_heads')\n",
    "experts_per_token = getattr(config,'num_experts_per_tok')\n",
    "hidden_size = getattr(config,'hidden_size')\n",
    "context_length = getattr(config,'max_position_embeddings')\n",
    "num_hidden_layers= getattr(config,'num_hidden_layers')\n",
    "\n",
    "print(f'context length : {context_length}')\n",
    "print(f'vocab size : {vocab_size}')\n",
    "print(f'embedding dim : {hidden_size}')\n",
    "print(f'total layers : {num_hidden_layers}')\n",
    "print(f'head size : {head_dim}')\n",
    "print(f'total key value heads : {key_heads}')\n",
    "print(f'total attention heads : {attn_heads}')\n",
    "print(f'key shape : {model.model.layers[0].self_attn.k_proj.weight.shape}')\n",
    "print(f'query shape : {model.model.layers[0].self_attn.q_proj.weight.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f234072a-94d3-449b-8021-12f67bc21409",
   "metadata": {},
   "source": [
    "## Understanding Harmony Format and Reasoning Levels\n",
    "\n",
    "OpenAI's GPT OSS 20B model supports the **Harmony format**, which is a structured approach to reasoning that makes the model's thought process visible and controllable. This format is particularly valuable for understanding how the model arrives at its conclusions.\n",
    "\n",
    "### What is Harmony Format?\n",
    "\n",
    "Harmony format wraps the model's reasoning process in special tags, allowing you to:\n",
    "\n",
    "1. **See the reasoning process**: Understand how the model thinks through problems\n",
    "2. **Control reasoning depth**: Adjust how much the model should reason before responding\n",
    "3. **Improve response quality**: More reasoning often leads to better, more thoughtful answers\n",
    "4. **Debug model behavior**: Identify where the model might be making errors in logic\n",
    "\n",
    "### Harmony Format Structure\n",
    "\n",
    "The basic structure uses reasoning tags to separate thinking from the final response:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8918766b-fbd3-48bc-9f73-a7d242e9662d",
   "metadata": {},
   "source": [
    "### Harmony Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "decc087f-0d77-423e-95b9-7930f5dc28ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example input\n",
    "harmony_prompt = \"\"\"\n",
    "<|start|>system<|message|>\n",
    "You are an agent who answers questions and uses tools when needed.\n",
    "<|end|>\n",
    "<|start|>developer<|message|>\n",
    "type get_weather = (_: {location: string}) => any;\n",
    "<|end|>\n",
    "<|start|>user<|message|>\n",
    "What is the weather in Tokyo?\n",
    "<|end|>\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(harmony_prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0540b6a8-73a4-4e70-bcc9-0c3f27273124",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:2489: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(**inputs, max_new_tokens=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "960be0d4-3449-4928-8a85-7511354802be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<|start|>system<|message|>\n",
      "You are an agent who answers questions and uses tools when needed.\n",
      "<|end|>\n",
      "<|start|>developer<|message|>\n",
      "type get_weather = (_: {location: string}) => any;\n",
      "<|end|>\n",
      "<|start|>user<|message|>\n",
      "What is the weather in Tokyo?\n",
      "<|end|>\n",
      "We need to generate a plan. The user asks \"What is the weather in Tokyo?\" We have a tool get_weather that can accept location string and returns some result. Thus we should call get_weather. Use the tool. \n",
      "We need to call the tool get_weather with location=\"Tokyo\". \n",
      "Then produce the response.<|end|><|start|>assistant<|channel|>analysis<|message|>We will call the tool.<|end|><|start|>assistant<|channel|>commentary to=functions.get_weather <|constrain|>json<|message|>{\"location\":\"Tokyo\"}<|call|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155c1a9d-09bd-4a87-a237-8023d0e6d921",
   "metadata": {},
   "source": [
    "### Harmony format- Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "289fa8c6-062e-41a9-a5ad-3acf7467bcc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <|start|>system<|message|>\n",
      "You are AutoBot, a helpful vehicle repair assistant. You will use the tools provided to you to get addional information.\n",
      "<|end|>\n",
      "\n",
      "<|start|>developer<|message|>\n",
      "type get_dtc_description = (_: {dtc_code: string}) => any;\n",
      "<|end|>\n",
      "\n",
      "<|start|>user<|message|>\n",
      "My car dashboard shows DTC error code P0301. What does it mean and how can I fix it?\n",
      "<|end|>\n",
      "We have user query: \"My car dashboard shows DTC error code P0301. What does it mean and how can I fix it?\"\n",
      "\n",
      "We need to provide explanation: P0301 is a misfire code for cylinder 1. Provide info on meaning, causes: spark plugs, wires, ignition coils, fuel issues, timing, vacuum leaks, etc. Provide steps to troubleshoot, fix.\n",
      "\n",
      "We also have tool get_dtc_description: we can call it with dtc_code: \"P0301\". Let's retrieve description.<|end|><|start|>assistant<|channel|>analysis to=tool.get_dtc_description code<|message|>{\"dtc_code\":\"P030\n"
     ]
    }
   ],
   "source": [
    "initial__prompt = \"\"\" <|start|>system<|message|>\n",
    "You are AutoBot, a helpful vehicle repair assistant. You will use the tools provided to you to get addional information.\n",
    "<|end|>\n",
    "\n",
    "<|start|>developer<|message|>\n",
    "type get_dtc_description = (_: {dtc_code: string}) => any;\n",
    "<|end|>\n",
    "\n",
    "<|start|>user<|message|>\n",
    "My car dashboard shows DTC error code P0301. What does it mean and how can I fix it?\n",
    "<|end|>\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(initial__prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "initial_response = tokenizer.decode(outputs[0])\n",
    "print(initial_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74904d3c-9c04-4191-9057-6206145225d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <|start|>system<|message|>\n",
      "You are AutoBot, a helpful vehicle repair assistant. You will use the tools provided to you to get addional information.\n",
      "<|end|>\n",
      "\n",
      "<|start|>developer<|message|>\n",
      "type get_dtc_description = (_: {dtc_code: string}) => any;\n",
      "<|end|>\n",
      "\n",
      "<|start|>user<|message|>\n",
      "My car dashboard shows DTC error code P0301. What does it mean and how can I fix it?\n",
      "<|end|>\n",
      "We have user query: \"My car dashboard shows DTC error code P0301. What does it mean and how can I fix it?\"\n",
      "\n",
      "We need to provide explanation: P0301 is a misfire code for cylinder 1. Provide info on meaning, causes: spark plugs, wires, ignition coils, fuel issues, timing, vacuum leaks, etc. Provide steps to troubleshoot, fix.\n",
      "\n",
      "We also have tool get_dtc_description: we can call it with dtc_code: \"P0301\". Let's retrieve description.<|end|><|start|>assistant<|channel|>analysis to=tool.get_dtc_description code<|message|>{\"dtc_code\":\"P030\n",
      "<|start|>tool<|message|>\n",
      "{ \"description\": \"P0301 indicates misfire detected in cylinder 1. Common causes include faulty spark plug, ignition coil, or fuel injector.\" } [commentary channel, from: get_dtc_description]\n",
      "<|end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tool_result = \"\"\"\n",
    "<|start|>tool<|message|>\n",
    "{ \"description\": \"P0301 indicates misfire detected in cylinder 1. Common causes include faulty spark plug, ignition coil, or fuel injector.\" } [commentary channel, from: get_dtc_description]\n",
    "<|end|>\n",
    "\"\"\"\n",
    "\n",
    "follow_prompt = initial_response + tool_result\n",
    "print(follow_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "629881db-9389-44e9-974e-0f226e281b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <|start|>system<|message|>\n",
      "You are AutoBot, a helpful vehicle repair assistant. You will use the tools provided to you to get addional information.\n",
      "<|end|>\n",
      "\n",
      "<|start|>developer<|message|>\n",
      "type get_dtc_description = (_: {dtc_code: string}) => any;\n",
      "<|end|>\n",
      "\n",
      "<|start|>user<|message|>\n",
      "My car dashboard shows DTC error code P0301. What does it mean and how can I fix it?\n",
      "<|end|>\n",
      "We have user query: \"My car dashboard shows DTC error code P0301. What does it mean and how can I fix it?\"\n",
      "\n",
      "We need to provide explanation: P0301 is a misfire code for cylinder 1. Provide info on meaning, causes: spark plugs, wires, ignition coils, fuel issues, timing, vacuum leaks, etc. Provide steps to troubleshoot, fix.\n",
      "\n",
      "We also have tool get_dtc_description: we can call it with dtc_code: \"P0301\". Let's retrieve description.<|end|><|start|>assistant<|channel|>analysis to=tool.get_dtc_description code<|message|>{\"dtc_code\":\"P030\n",
      "<|start|>tool<|message|>\n",
      "{ \"description\": \"P0301 indicates misfire detected in cylinder 1. Common causes include faulty spark plug, ignition coil, or fuel injector.\" } [commentary channel, from: get_dtc_description]\n",
      "<|end|>\n",
      "We can use that. Use the description.\n",
      "\n",
      "But we need to produce final answer.<|end|><|start|>assistant<|channel|>final<|message|>**P0301 – Cylinder 1 Misfire**  \n",
      "The code P0301 tells the on‑board computer that the engine is misfiring in **cylinder 1**.  \n",
      "A misfire means the air‑fuel mixture in that spark plug’s combustion chamber isn’t igniting properly or the ignition doesn’t last long enough.\n",
      "\n",
      "---\n",
      "\n",
      "## Why Happens\n",
      "\n",
      "| Likely Cause | How it shows up |\n",
      "|--------------|-----------------|\n",
      "| **Faulty spark plug** (old, cracked, fouled)\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(follow_prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "final_response = tokenizer.decode(outputs[0])\n",
    "print(final_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33aa0d8-c58b-4140-bfcc-7dc5086d6152",
   "metadata": {},
   "source": [
    "### Using Harmony renderer library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83f2507d-27ed-47c5-880b-17797b08989a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai-harmony in /opt/conda/lib/python3.10/site-packages (0.0.4)\n",
      "Requirement already satisfied: pydantic>=2.11.7 in /opt/conda/lib/python3.10/site-packages (from openai-harmony) (2.11.7)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.11.7->openai-harmony) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.11.7->openai-harmony) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.11.7->openai-harmony) (4.14.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.11.7->openai-harmony) (0.4.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai-harmony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "784ca2bc-c077-4597-8203-1be3d5f5cdb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:2489: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
      "Knowledge cutoff: 2024-06\n",
      "Current date: 2025-08-26\n",
      "\n",
      "Reasoning: low\n",
      "\n",
      "# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions\n",
      "\n",
      "You are a highly skilled vehicle repair assistant. Diagnose car problems, suggest solutions, and explain your reasoning clearly. Always ask clarifying questions if the user's description is vague. If the issue might be dangerous, warn the user and advise professional inspection.<|end|><|start|>user<|message|>My 2013 Honda Accord is making a rattling noise when I accelerate, and the check engine light is on. What could be the problem?<|end|><|start|>assistant<|channel|>analysis<|message|>We need to ask clarifying questions.<|end|><|start|>assistant<|channel|>final<|message|>I’m glad you reached out—rattling noises during acceleration can mean a few things, especially when the Check Engine Light (CEL) is on. Let’s narrow it down so you can decide whether it’s a quick fix or something that needs a shop visit.\n",
      "\n",
      "### 1. Clarifying Questions\n",
      "1. **When does the rattling start?**  \n",
      "   - Does it happen as soon as you lift off the brake, or does it wait a few seconds?\n",
      "2. **How long does the rattling last?**  \n",
      "   - Does it stop after a few seconds, or does it persist through the acceleration range?\n",
      "3. **What does the rattling sound like?**  \n",
      "   - Is it a “clunk,” “tap,” “twang,” or a more high‑pitched “banging” noise?\n",
      "4. **Vehicle Conditions**  \n",
      "   - Is the engine warm or cold when you notice it?  \n",
      "   - Is the fuel level high or low?\n",
      "5. **Check Engine Light Data**  \n",
      "   - Have you retrieved the diagnostic trouble codes (DTCs) from the ECU? If not, a quick scan with a code reader (or the free app on many smartphones) can reveal the exact codes.\n",
      "6. **Other Symptoms**  \n",
      "   - Do you feel any loss of power, hesitation, or a rough idle?  \n",
      "   - Are there any other warning lights or warning messages on the dash?\n",
      "\n",
      "---\n",
      "\n",
      "### 2. Common Causes (Based on Your Description)\n",
      "\n",
      "| # | Possible Cause | Why It Might Fit | Typical Fix |\n",
      "|---|----------------|------------------|-------------|\n",
      "| 1 | **Loose or Worn Exhaust Heat‑shield or Component** | Heat‑shields can rattle when engine gets hot, usually noticeable during acceleration because the exhaust back‑pressure changes. | Tighten or replace the heat‑shield, clamps, or brackets. |\n",
      "| 2 | **Muffler or Exhaust System Leak** | A loose connection or damaged muffler can “bounce” on vibrations, especially when the engine output ramps up. | Check for loose exhaust hangers, clamps, or cracked muffler sections. |\n",
      "| 3 | **Loose Engine‑Mount or Related Accessories** | Engine mounts allow the engine to shift slightly. On acceleration, the shift can create a rattling “clunk.” | Inspect mounts for cracks or broken bushings; replace if needed. |\n",
      "| 4 | **Transmission or Differential Mount Issue** | Similar to engine mounts—especially if the problem seems to change with gear changes. | Inspect mounts and replace if damaged. |\n",
      "| 5 | **Defective Turbo / Boost‑related Leak (2013 Accord F‑Sport)** | This model has a small turbocharger. A loose turbo shaft or intercooler hose can rattle in the turbo housing when the boost rises. | Inspect turbo housing, bushings, and hoses. |\n",
      "| 6 | **Accessory Belt or Pulley Problem** | A worn serpentine belt or a loose pulley can produce a “squeak” that turns into a rattling noise at higher RPMs. | Check belt tension and pulley bearings. |\n",
      "| 7 | **Intake System or Air‑Fuel Mixing Issues** | Misfire or unburned fuel can cause “firing” sounds that might be perceived as rattling. | Check engine codes for misfire codes (`P030X`), inspect spark plugs, ignition coils, and fuel injectors. |\n",
      "| 8 | **Engine Cooling System (Water Pump, Hoses)** | A loose or leaking hose might vibrate when heated. | Inspect hoses for cracks, loose clamps, and the water pump for noise. |\n",
      "| 9 | **Suspension or Chassis** | Rare, but a loose lower control arm or sway bar link can shift under load during acceleration, making a rattle. | Inspect the suspension hardware, especially on the front lower control arms. |\n",
      "\n",
      "---\n",
      "\n",
      "### 3. Immediate Safety and Preventive Steps\n",
      "\n",
      "1. **Check the DTCs** – If you have a code reader, pull the codes now. If it’s a misfire (`P0300–P0308`) or an emission‑related problem, that could explain the CEL.\n",
      "2. **Don’t Ignore the CEL** – Even if the noise seems “harmless,” a CEL often indicates an issue that could worsen or lead to a more serious failure.\n",
      "3. **If the Noise is Harsh or Gets Worse** – Stop driving the car. Prolonged operation with a component that is rattling—especially if it's in the exhaust, transmission, or engine assembly—could damage the component or increase emissions, which could be hazardous in some environments.\n",
      "4. **Avoid DIY Replacements on Critical Safety Parts** – Engine mounts, transmission mounts, and turbo housings are vital for vehicle stability and safety. If you’re not sure, it’s best to have a professional handle them.\n",
      "\n",
      "---\n",
      "\n",
      "### \n"
     ]
    }
   ],
   "source": [
    "from openai_harmony import (\n",
    "    Author,\n",
    "    Conversation,\n",
    "    DeveloperContent,\n",
    "    HarmonyEncodingName,\n",
    "    Message,\n",
    "    Role,\n",
    "    SystemContent,\n",
    "    load_harmony_encoding,\n",
    "    ReasoningEffort,\n",
    ")\n",
    "\n",
    "reason_level = ReasoningEffort.LOW\n",
    "\n",
    "# Load harmony encoding for GPT OSS\n",
    "encoding = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)\n",
    "\n",
    "# Define the system message with assistant persona and behavior\n",
    "system_message = SystemContent.new().with_reasoning_effort(reason_level).with_conversation_start_date(\"2025-08-26\")\n",
    "\n",
    "# Add developer instructions emphasizing automotive repair expertise\n",
    "developer_message = DeveloperContent.new().with_instructions(\n",
    "    \"You are a highly skilled vehicle repair assistant. Diagnose car problems, suggest solutions, and explain your reasoning clearly. \" \\\n",
    "    \"Always ask clarifying questions if the user's description is vague. If the issue might be dangerous, warn the user and advise professional inspection.\"\n",
    ")\n",
    "\n",
    "# Start user interaction\n",
    "user_message = \"My 2013 Honda Accord is making a rattling noise when I accelerate, and the check engine light is on. What could be the problem?\"\n",
    "\n",
    "# Compose the conversation history as expected by the model\n",
    "convo = Conversation.from_messages([\n",
    "    Message.from_role_and_content(Role.SYSTEM, system_message),\n",
    "    Message.from_role_and_content(Role.DEVELOPER, developer_message),\n",
    "    Message.from_role_and_content(Role.USER, user_message)\n",
    "]\n",
    ")\n",
    "\n",
    "# Render this conversation into the prompt format for model inference\n",
    "prompt = encoding.render_conversation_for_completion(convo, Role.ASSISTANT)\n",
    "input_ids = torch.tensor([prompt])\n",
    "outputs = model.generate(input_ids, max_new_tokens=1028)\n",
    "completion = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b10c0cde-a31b-49d3-91c9-7d77a2ec2322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
      "Knowledge cutoff: 2024-06\n",
      "Current date: 2025-08-26\n",
      "\n",
      "Reasoning: low\n",
      "\n",
      "# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions\n",
      "\n",
      "You are a highly skilled vehicle repair assistant. Diagnose car problems, suggest solutions, and explain your reasoning clearly. Always ask clarifying questions if the user's description is vague. If the issue might be dangerous, warn the user and advise professional inspection.<|end|><|start|>user<|message|>My 2013 Honda Accord is making a rattling noise when I accelerate, and the check engine light is on. What could be the problem?<|end|><|start|>assistant\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, list)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokenizer.decode(prompt)), type(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c20154a-56df-4df9-8191-68b5c8650aa4",
   "metadata": {},
   "source": [
    "### Reasoning Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b42de0a9-7336-4369-b05e-e18043caffe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning effort :medium\n",
      "Response: \n",
      " <|start|>system<|message|>You are OpenAI GPT OSS.\n",
      "Knowledge cutoff: 2024-06\n",
      "Current date: 2025-08-26\n",
      "\n",
      "Reasoning: medium\n",
      "\n",
      "# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions\n",
      "\n",
      "This will actually become a developer message!\n",
      "\n",
      "<|end|><|start|>user<|message|>Why is my car making a clicking sound when I turn the key?<|end|><|start|>assistant<|channel|>analysis<|message|>We have a user question: \"Why is my car making a clicking sound when I\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "reason_levels = ['high','medium','low']\n",
    "reason_effort = random.choice(reason_levels)\n",
    "\n",
    "print(f'Reasoning effort :{reason_effort}')\n",
    "\n",
    "chat = [\n",
    "    {\"role\": \"system\", \"content\": \"This will actually become a developer message!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Why is my car making a clicking sound when I turn the key?\"}\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    chat, \n",
    "    model_identity=\"You are OpenAI GPT OSS.\",\n",
    "    reasoning_effort=reason_effort,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model.generate(**inputs)\n",
    "response = tokenizer.decode(outputs[0])\n",
    "print(f'Response: \\n {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533e80cf-324c-43e9-a3b7-6eccff2db37e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
